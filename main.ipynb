{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 21:06:30.767081: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-07 21:06:31.020824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 21:06:31.020963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 21:06:31.055525: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 21:06:31.130667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 21:06:31.933772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import permutation\n",
    "np.random.seed(2016)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPool2D # GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from keras import regularizers\n",
    "import h5py\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, \\\n",
    "    GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Conv2D\n",
    "from keras.models import Model\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.utils import get_source_inputs, plot_model\n",
    "# from depthwise_conv2d import DepthwiseConvolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = 1\n",
    "dataset_path = r'../Project/Dataset/'\n",
    "np.random.seed(2016)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(src=img, dsize=(224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    df = pd.read_csv(dataset_path + 'auc.distracted.driver.train.csv')\n",
    "    x = df.iloc[:,0]\n",
    "    y = df.iloc[:,1]\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    print('Read train images')\n",
    "    for i in range (0,len(x)):\n",
    "        fl= dataset_path + r'v1_cam1_no_split/' + r'/'.join(x[i].split('/')[-2:])\n",
    "        print(f\"Reading image {i}/{len(x)}\")\n",
    "        img = get_im_cv2(fl)\n",
    "        X_train.append(img)\n",
    "        Y_train.append(y[i])\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid():\n",
    "    df = pd.read_csv(dataset_path + 'auc.distracted.driver.test.csv')\n",
    "    x = df.iloc[:,0]\n",
    "    y = df.iloc[:,1]\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "    print('Read test images')\n",
    "    for i in range (0,len(x)):\n",
    "        fl = dataset_path + r'v1_cam1_no_split/' + r'/'.join(x[i].split('/')[-2:])\n",
    "        print(f\"Reading image {i}/{len(x)}\")\n",
    "        img = get_im_cv2(fl)\n",
    "        X_valid.append(img)\n",
    "        Y_valid.append(y[i])\n",
    "    return X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    cache_path = r'../Project/Dataset/cache/train_v1.dat'\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target= load_train()\n",
    "        cache_data((train_data, train_target), cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target) = restore_data(cache_path)\n",
    "    \n",
    "    print('Convert to numpy...')\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "    \n",
    "    print('Reshape...')\n",
    "    train_data = train_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    # Normalise the train data\n",
    "    print('Convert to float...')\n",
    "    train_data = train_data.astype('float16')\n",
    "    mean_pixel = [80.857, 81.106, 82.928]\n",
    "    \n",
    "    print('Substract 0...')\n",
    "    train_data[:, :, :, 0] -= mean_pixel[0]\n",
    "    \n",
    "    print('Substract 1...')\n",
    "    train_data[:, :, :, 1] -= mean_pixel[1]\n",
    "\n",
    "    print('Substract 2...')\n",
    "    train_data[:, :, :, 2] -= mean_pixel[2]\n",
    "\n",
    "    train_target = utils.to_categorical(train_target, 10)\n",
    "    \n",
    "    # Shuffle experiment START !!\n",
    "    perm = permutation(len(train_target))\n",
    "    train_data = train_data[perm]\n",
    "    train_target = train_target[perm]\n",
    "    # Shuffle experiment END !!\n",
    "    \n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    print('Target shape:', train_target.shape)\n",
    "    print(train_target.shape[0], 'target samples')\n",
    "    return train_data, train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_test_data():\n",
    "    start_time = time.time()\n",
    "    cache_path = r'../Project/Dataset/cache/test_v1.dat'\n",
    "\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_target = load_valid()\n",
    "        cache_data((test_data, test_target ), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache [{}]!')\n",
    "        (test_data, test_target) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    # Normalise the test data data\n",
    "\n",
    "    test_data = test_data.astype('float16')\n",
    "    mean_pixel = [80.857, 81.106, 82.928]\n",
    "\n",
    "    test_data[:, :, :, 0] -= mean_pixel[0]\n",
    "\n",
    "    test_data[:, :, :, 1] -= mean_pixel[1]\n",
    "\n",
    "    test_data[:, :, :, 2] -= mean_pixel[2]\n",
    "\n",
    "    test_target = utils.to_categorical(test_target, 10)\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    print('Target shape:', test_target.shape)\n",
    "    print(test_target.shape[0], 'target samples')\n",
    "    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore train from cache!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to numpy...\n",
      "Reshape...\n",
      "Convert to float...\n",
      "Substract 0...\n",
      "Substract 1...\n",
      "Substract 2...\n",
      "Train shape: (12977, 224, 224, 3)\n",
      "12977 train samples\n",
      "Target shape: (12977, 10)\n",
      "12977 target samples\n",
      "Restore test from cache [{}]!\n",
      "Test shape: (4331, 224, 224, 3)\n",
      "4331 test samples\n",
      "Target shape: (4331, 10)\n",
      "4331 target samples\n",
      "Read and process test data time: 5.33 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_and_normalize_train_data()\n",
    "X_valid, Y_valid = read_and_normalize_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_16(input_shape=(224,224,3), classes=10):\n",
    "\n",
    "    img_input = Input(shape =input_shape)\n",
    "    print(img_input)\n",
    "\n",
    "    # 1st Conv Block\n",
    "\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(img_input)\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    \n",
    "    # 2nd Conv Block\n",
    "\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    \n",
    "    # 3rd Conv block\n",
    "\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    # 4th Conv block\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # 5th Conv block\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x= Dropout(0.3)(x)\n",
    "\n",
    "    # Last block\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =7, padding ='same', activation='relu')(x)\n",
    "    x= Dropout(0.4)(x)\n",
    "    x = Conv2D (filters =512, kernel_size =1, padding ='same', activation='relu')(x)\n",
    "    x= Dropout(0.5)(x)\n",
    "    x = Conv2D (filters =10, kernel_size =1, padding ='same', activation='softmax')(x)\n",
    "    output = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # creating the model\n",
    "\n",
    "    model = Model (inputs=img_input, outputs =output, name='VGG_16')\n",
    "    model.summary()    \n",
    "\n",
    "    # model.load_weights('weights.h5')\n",
    "    opt = SGD(learning_rate=0.0001, momentum=0.9, weight_decay=10e-6)\n",
    "\n",
    "    model.compile(opt, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n",
      "Model: \"VGG_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 112, 112, 64)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 512)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 512)         12845568  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 7, 7, 512)         262656    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 7, 7, 10)          5130      \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 10)                0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27828042 (106.16 MB)\n",
      "Trainable params: 27828042 (106.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 21:06:56.088949: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.162927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.163012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.163918: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.163977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.164018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.238408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.238493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.238543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-07 21:06:56.238583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20424 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = VGG_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path=r'./Checkpoint/'       \n",
    "callbacks = [ModelCheckpoint(weights_path, monitor='val_acc', save_best_only=True, verbose=1)]\n",
    "\n",
    "print(X_train.shape)\n",
    "hist = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_valid, Y_valid), use_multiprocessing=True, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(hist.history).to_csv(r'../Project/Dataset/cache/try_hist.csv')\n",
    "\n",
    "predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=1)\n",
    "cm1=confusion_matrix(Y_valid.argmax(axis=1), predictions_valid.argmax(axis=1))\n",
    "ss=cm1[0,0]+cm1[1,1]+cm1[2,2]+cm1[3,3]+cm1[4,4]+cm1[5,5]+cm1[6,6]+cm1[7,7]+cm1[8,8]+cm1[9,9];\n",
    "test_accuracy=np.divide(ss,4331);\n",
    "print('Test Accuracy:',test_accuracy)\n",
    "\n",
    "ppath=os.path.join('/home/gpu3/Desktop/mobileVGG','cache','confusion_mat.npy')\n",
    "np.save(ppath, cm1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
