{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "import cv2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import permutation\n",
    "np.random.seed(2016)\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, MaxPool2D # GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras import utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from keras import regularizers\n",
    "import h5py\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Convolution2D, \\\n",
    "    GlobalAveragePooling2D, Dense, BatchNormalization, Activation, Conv2D\n",
    "from keras.models import Model\n",
    "# from keras.engine.topology import get_source_inputs\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.utils import get_source_inputs, plot_model\n",
    "# from depthwise_conv2d import DepthwiseConvolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = 1\n",
    "dataset_path = r'../Dataset/v2_cam1_cam2'\n",
    "\n",
    "np.random.seed(1234)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im_cv2(path):\n",
    "    img = cv2.imread(path)\n",
    "    resized = cv2.resize(src=img, dsize=(224, 224), interpolation=cv2.INTER_LINEAR) / 255.0\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(num_classes=10):\n",
    "    train_dir_cam1 = dataset_path + '/Camera 1/train/'\n",
    "    train_dir_cam2 = dataset_path + '/Camera 2/train/'\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    print('Read train images')\n",
    "\n",
    "    for label in range(num_classes):\n",
    "        class_label = 'c' + str(label)  # Example: c0\n",
    "        class_dir_cam1 = train_dir_cam1 + class_label\n",
    "        class_dir_cam2 = train_dir_cam2 + class_label\n",
    "\n",
    "        x1 = os.listdir(class_dir_cam1)\n",
    "        x2 = os.listdir(class_dir_cam2)\n",
    "\n",
    "        for i in range (0,len(x1)):\n",
    "            print(f\"Reading image of camera 1 - class {class_label}: {i}/{len(x1)}\")\n",
    "            img = get_im_cv2(class_dir_cam1 + '/' + x1[i])\n",
    "            X_train.append(img)\n",
    "            Y_train.append(label)\n",
    "\n",
    "        for i in range (0,len(x2)):\n",
    "            print(f\"Reading image of camera 2 - class {class_label}: {i}/{len(x2)}\")\n",
    "            img = get_im_cv2(class_dir_cam2 + '/' + x2[i])\n",
    "            X_train.append(img)\n",
    "            Y_train.append(label)\n",
    "\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid(num_classes=10):\n",
    "    test_dir_cam1 = dataset_path + '/Camera 1/test/'\n",
    "    test_dir_cam2 = dataset_path + '/Camera 2/test/'\n",
    "\n",
    "    X_valid = []\n",
    "    Y_valid = []\n",
    "\n",
    "    print('Read test images')\n",
    "\n",
    "    for label in range(num_classes):\n",
    "        class_label = 'c' + str(label)  # Example: c0\n",
    "        class_dir_cam1 = test_dir_cam1 + class_label\n",
    "        class_dir_cam2 = test_dir_cam2 + class_label\n",
    "\n",
    "        x1 = os.listdir(class_dir_cam1)\n",
    "        x2 = os.listdir(class_dir_cam2)\n",
    "\n",
    "        for i in range (0,len(x1)):\n",
    "            print(f\"Reading image of camera 1 - class {class_label}: {i}/{len(x1)}\")\n",
    "            img = get_im_cv2(class_dir_cam1 + '/' + x1[i])\n",
    "            X_valid.append(img)\n",
    "            Y_valid.append(label)\n",
    "\n",
    "        for i in range (0,len(x2)):\n",
    "            print(f\"Reading image of camera 2 - class {class_label}: {i}/{len(x2)}\")\n",
    "            img = get_im_cv2(class_dir_cam2 + '/' + x2[i])\n",
    "            X_valid.append(img)\n",
    "            Y_valid.append(label)\n",
    "\n",
    "\n",
    "    return X_valid, Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    if os.path.isdir(os.path.dirname(path)):\n",
    "        file = open(path, 'wb')\n",
    "        pickle.dump(data, file)\n",
    "        file.close()\n",
    "    else:\n",
    "        print('Directory doesnt exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(path):\n",
    "    data = dict()\n",
    "    if os.path.isfile(path):\n",
    "        file = open(path, 'rb')\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_train_data():\n",
    "    cache_path = dataset_path + 'cache/train_v1.dat'\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        train_data, train_target= load_train()\n",
    "        cache_data((train_data, train_target), cache_path)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (train_data, train_target) = restore_data(cache_path)\n",
    "    \n",
    "    print('Convert to numpy...')\n",
    "    train_data = np.array(train_data, dtype=np.uint8)\n",
    "    train_target = np.array(train_target, dtype=np.uint8)\n",
    "    \n",
    "    print('Reshape...')\n",
    "    train_data = train_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    # Normalise the train data\n",
    "    print('Convert to float...')\n",
    "    train_data = train_data.astype('float16')\n",
    "    mean_pixel = [80.857, 81.106, 82.928]\n",
    "    \n",
    "    print('Substract 0...')\n",
    "    train_data[:, :, :, 0] -= mean_pixel[0]\n",
    "    \n",
    "    print('Substract 1...')\n",
    "    train_data[:, :, :, 1] -= mean_pixel[1]\n",
    "\n",
    "    print('Substract 2...')\n",
    "    train_data[:, :, :, 2] -= mean_pixel[2]\n",
    "\n",
    "    train_target = utils.to_categorical(train_target, 10)\n",
    "    \n",
    "    # Shuffle experiment START !!\n",
    "    perm = permutation(len(train_target))\n",
    "    train_data = train_data[perm]\n",
    "    train_target = train_target[perm]\n",
    "    # Shuffle experiment END !!\n",
    "    \n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    print('Target shape:', train_target.shape)\n",
    "    print(train_target.shape[0], 'target samples')\n",
    "    return train_data, train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_normalize_test_data():\n",
    "    start_time = time.time()\n",
    "    cache_path = dataset_path + 'cache/test_v1.dat'\n",
    "\n",
    "    if not os.path.isfile(cache_path) or use_cache == 0:\n",
    "        test_data, test_target = load_valid()\n",
    "        cache_data((test_data, test_target ), cache_path)\n",
    "    else:\n",
    "        print('Restore test from cache [{}]!')\n",
    "        (test_data, test_target) = restore_data(cache_path)\n",
    "\n",
    "    test_data = np.array(test_data, dtype=np.uint8)\n",
    "    test_data = test_data.transpose((0, 1, 2, 3))\n",
    "\n",
    "    # Normalise the test data data\n",
    "\n",
    "    test_data = test_data.astype('float16')\n",
    "    mean_pixel = [80.857, 81.106, 82.928]\n",
    "\n",
    "    test_data[:, :, :, 0] -= mean_pixel[0]\n",
    "\n",
    "    test_data[:, :, :, 1] -= mean_pixel[1]\n",
    "\n",
    "    test_data[:, :, :, 2] -= mean_pixel[2]\n",
    "\n",
    "    test_target = utils.to_categorical(test_target, 10)\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    print('Target shape:', test_target.shape)\n",
    "    print(test_target.shape[0], 'target samples')\n",
    "    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return test_data, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_and_normalize_train_data()\n",
    "X_valid, Y_valid = read_and_normalize_test_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
